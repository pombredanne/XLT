package com.xceptance.xlt.report;

import java.util.ArrayList;
import java.util.List;
import java.util.Map.Entry;
import java.util.concurrent.ExecutionException;
import java.util.concurrent.ExecutorService;
import java.util.concurrent.Executors;
import java.util.concurrent.Future;
import java.util.regex.Matcher;
import java.util.regex.Pattern;

import org.apache.commons.logging.Log;
import org.apache.commons.logging.LogFactory;

import com.xceptance.common.util.SimpleArrayList;
import com.xceptance.xlt.api.engine.ActionData;
import com.xceptance.xlt.api.engine.Data;
import com.xceptance.xlt.api.engine.PageLoadTimingData;
import com.xceptance.xlt.api.engine.RequestData;
import com.xceptance.xlt.api.engine.TransactionData;
import com.xceptance.xlt.report.mergerules.RequestProcessingRule;
import com.xceptance.xlt.report.mergerules.RequestProcessingRuleResult;

/**
 * Parses lines to data records and performs any data record preprocessing that can be done in parallel. Preprocessing
 * also includes executing request merge rules.
 */
class DataParserThread implements Runnable
{
    /**
     * Class logger.
     */
    private static final Log LOG = LogFactory.getLog(DataParserThread.class);

    /**
     * Pattern used to rename the name of Web driver timers generated by FF add-on.
     */
    private static final Pattern WD_TIMER_NAME_PATTERN = Pattern.compile("page_\\d+");

    /**
     * The data record factory.
     */
    private final DataRecordFactory dataRecordFactory;

    /**
     * The dispatcher that coordinates result processing.
     */
    private final Dispatcher dispatcher;

    /**
     * The start time of the report period. Data records generated outside this window will be ignored.
     */
    private final long fromTime;

    /**
     * The end time of the report period. Data records generated outside this window will be ignored.
     */
    private final long toTime;

    /**
     * The general config of the report generator
     */
    private final ReportGeneratorConfiguration config;
    
    final static ExecutorService pool = Executors.newWorkStealingPool(8);

    
    /**
     * Constructor.
     *
     * @param dataRecordFactory
     *            the data record factory
     * @param fromTime
     *            the start time
     * @param toTime
     *            the end time
     * @param requestProcessingRules
     *            the request processing rules
     * @param dispatcher
     *            the dispatcher that coordinates result processing
     * @param removeIndexesFromRequestNames
     *            whether to automatically remove any indexes from request names
     */
    public DataParserThread(final Dispatcher dispatcher, final DataRecordFactory dataRecordFactory, final long fromTime, final long toTime,
                            final ReportGeneratorConfiguration config)
    {
        this.dataRecordFactory = dataRecordFactory;
        this.fromTime = fromTime;
        this.toTime = toTime;
        this.dispatcher = dispatcher;
        this.config = config;
    }

    /**
     * {@inheritDoc}
     */
    @Override
    public void run()
    {
        final List<RequestProcessingRule> requestProcessingRules = config.getRequestProcessingRules();
        final boolean removeIndexes = config.getRemoveIndexesFromRequestNames();
        
        while (true)
        {
            try
            {
                // get a chunk of lines
                final DataChunk lineChunk = dispatcher.retrieveReadData();
                final List<String> lines = lineChunk.getLines();

                // parse the chunk of lines and preprocess the results
                final SimpleArrayList<Data> dataRecordChunk = new SimpleArrayList<>(lines.size());
                final List<Future<RequestData>> tasks = new ArrayList<>();
                
                int lineNumber = lineChunk.getBaseLineNumber();

                final int size = lines.size();
                for (int i = 0; i < size; i++)
                {
                    final Data data = parseLine(lines.get(i), lineNumber, lineChunk);
                    if (data != null)
                    {
                        if (data instanceof RequestData)
                        {
                            // send it into parallel processing
                            Future<RequestData> result = pool.submit(() -> postprocess((RequestData) data, 
                                                                                       requestProcessingRules, 
                                                                                       removeIndexes));
                            tasks.add(result);
                            
//                            // send it into parallel processing
//                            RequestData result = postprocess((RequestData) data, requestProcessingRules, removeIndexes);
//                            if (result != null) dataRecordChunk.add(result);
                        }
                        else
                        {
                            dataRecordChunk.add(data);
                        }
                    }

                    lineNumber++;
                }

                tasks.stream().map(f -> {
                    try
                    {
                        return f.get();
                    }
                    catch (InterruptedException e)
                    {
                        LOG.error("Postprocessing was interrupted");
                    }
                    catch (ExecutionException e)
                    {
                        LOG.error("Postprocessing failed", e.getCause());
                    }
                    
                    return null;
                }).filter(d -> d != null).forEach(d -> dataRecordChunk.add(d));

                
                // deliver the chunk of parsed data records
                dispatcher.addPostprocessedData(dataRecordChunk);
            }
            catch (final InterruptedException e)
            {
                break;
            }
        }
    }

    /**
     * Parses the given line to a data record.
     *
     * @param line
     *            the line to parse
     * @param lineNumber
     *            the number of the line in its file (for logging purposes)
     * @param lineChunk
     *            the line chunk the line belongs to
     * @return the parsed data record, or <code>null</code> if the line could not be parsed or the data record's
     *         timestamp was outside the configured time period
     */
    private Data parseLine(final String line, final int lineNumber, final DataChunk lineChunk)
    {
        try
        {
            // parse the data record
            final Data dataRecord = dataRecordFactory.createStatistics(line);

            // skip the data record if it was not generated in the given time period
            final long time = dataRecord.getTime();
            if (time < fromTime || time > toTime)
            {
                return null;
            }

            // set general fields
            dataRecord.setAgentName(lineChunk.getAgentName());
            dataRecord.setTransactionName(lineChunk.getTestCaseName());

            // set special fields / special handling
            if (dataRecord instanceof TransactionData)
            {
                final TransactionData td = (TransactionData) dataRecord;
                td.setTestUserNumber(lineChunk.getUserNumber());
            }
            else if (lineChunk.getCollectActionNames() && dataRecord instanceof ActionData)
            {
                // store the action name/time for later use
                lineChunk.getActionNames().put(dataRecord.getTime(), dataRecord.getName());
            }
            else if (lineChunk.getAdjustTimerNames() && (dataRecord instanceof RequestData || dataRecord instanceof PageLoadTimingData))
            {
                // rename web driver requests/custom timers using the previously stored action names
                final Entry<Long, String> entry = lineChunk.getActionNames().floorEntry(time);
                final String actionName = (entry != null) ? entry.getValue() : "UnknownAction";

                final Matcher m = WD_TIMER_NAME_PATTERN.matcher(dataRecord.getName());
                dataRecord.setName(m.replaceFirst(actionName));
            }

            return dataRecord;
        }
        catch (final Exception ex)
        {
            final String msg = String.format("Failed to parse data record at line %,d in file '%s': %s", lineNumber, lineChunk.getFile(),
                                             ex);
            LOG.error(msg);
            ex.printStackTrace();

            return null;
        }
    }
    
    /**
     * Processes a request according to the configured request processing rules. Currently, this means renaming or
     * discarding requests.
     *
     * @param requestData
     *            the request data record
     * @param removeIndexesFromRequestNames 
     *              in case we want to clean the name too
     * @return the processed request data record, or <code>null</code> if the data record is to be discarded
     */
    private RequestData postprocess(RequestData requestData, 
                                           final List<RequestProcessingRule> requestProcessingRules, 
                                           boolean removeIndexesFromRequestNames)
    {
        // fix up the name first (Product.1.2 -> Product) if so configured
        if (removeIndexesFromRequestNames)
        {
            // @TODO Chance for more performance here
            String requestName = requestData.getName();

            final int firstDotPos = requestName.indexOf(".");
            if (firstDotPos > 0)
            {
                requestName = requestName.substring(0, firstDotPos);
                requestData.setName(requestName);
            }
        }

        // remember the original name so we can restore it in case request processing fails
        final String originalName = requestData.getName();

        // execute all processing rules one after the other until processing is complete
        final int size = requestProcessingRules.size();
        for (int i = 0; i < size; i++)
        {
            final RequestProcessingRule requestProcessingRule = requestProcessingRules.get(i);

            try
            {
                final RequestProcessingRuleResult result = requestProcessingRule.process(requestData);

                requestData = result.requestData;

                if (result.stopRequestProcessing)
                {
                    break;
                }
            }
            catch (final Throwable t)
            {
                final String msg = String.format("Failed to apply request merge rule: %s\n%s", requestProcessingRule, t);
                LOG.error(msg);

                // restore the request's original name
                requestData.setName(originalName);

                break;
            }
        }

        return requestData;
    }

}
